# 词典挖掘

## 算法步骤

- 找出高频词
- 训练分类器
- 用分类器对词语进行初步打分
- 迭代
	- Dp求出得分最高的分词方式
	- 根据每个词的使用频率更新词语的打分
- 利用最终的打分将词语排序
- 分别根据 `TF-IDF` 和 `Textrank` 给每篇文章求摘要

## 算法实现

### 找出高频词

此部分算法采用了论文 `Mining Quality Phrases from Massive Text Corpora` 中的算法，即一开始认为所有出现次数超过阈值的单个汉字为高频词，在迭代过程中，计算每个高频词与右边第一个汉字拼接后是否仍是高频词，如是，则向高频词集合中添加该新词。

### 训练分类器

该分类器的目的是初步判断给定词语是否是一个完整的词语。

用于训练分类器的数据是从高频词集合中随意抽取的约400个词，经手动标记后得到的。其中 $85\%$ 用于训练，$15\%$ 用于检测训练效果。

对于每个词语，用于分类的参数有：

- $PMI(s)=\min_{u\oplus v=s} \log\frac{p(s)}{p(u)p(v)}$：最小点互信息，即将一个词语拆分成前后两部分后，两部分的最小互信息。该值要是很小，意味着这个词是有两个以上的单独词语拼接而成的，不能作为一个单独的词语出现。
- $CMI(s)=\max_{u\oplus s=v}\log \frac{p^2(v)}{p(u)p(s)}$：最大点互信息，即将一个词语与相邻的词语的最大点互信息。该值要是很大，意味着这个词是它的某个母串的无意义子串，不能作为一个单独的词语出现。
- $RFC(s)=\frac{p(u)}{\sum_v p(v)}$：标准化的出现频率，即一个词语出现次数占所有词语的出现次数的比。
- $PKL(s)=p(s)PMI(s)$：论文中提到的一个奇怪的量。
- $IDF(s)=\log\frac{|D|}{1+\sum_{s\in d\in D}1}$：逆文档频率，即所有文档数与该词语出现过的文档数的比。

用这些参数训练出的分类器的准确度为 $0.84$ ，不是很高，但基本够用。

### 迭代

迭代过程分为两个部分：DP和重打分。

- Dp：将整个文本按照词语的得分以及词语的长度两个评价指标，使得划分后的指标之和尽量大。
- 重打分：每个词语的得分修正为原始的分加上：Dp中该词语使用次数与词语在整个文档集合中出现的总次数的比。然后将得分标准化，即保证所有词语的得分总和在整个迭代过程中不变。

迭代完成后得分前几的词语基本能看。但是仍然存在大量分的不完整的词语，比如“某某的，的某某”。

### TFIDF

该部分进行文档摘要时（肉眼评估）效果不是很好，因此我将评价指标修改为：`TF-IDF-Q`，即在 `TF-IDF` 的基础上再乘了词语得分。这样修正之后（肉眼评估）的效果变好了。至于如何量化的衡量效果好不好，我还没想到。

### Text Rank

该部分进行文档摘要时，效果就更不好了，总是摘要出一大堆单个字的词语。不知道是不是什么地方写错了，总之程序跑了这么久结果还这么差劲实在让人心情很不好。

## 收货

- 实现了词典挖掘算法 (`main.py`)
- 实现了基于 `TFIDF` 的摘要系统 (`main.py`)
  - 见所有以 `result` 打头的文件夹下的 `Abs.txt`
- 实现了基于 `Text Rank` 的摘要系统，只不过效果极差 (`modules/TextRank.py`)
  - 见所有以 `result` 打头的文件夹下的 `Rnk.txt`
- 实现了基于 `Jaccard` 系数的文本相似度计算 (`Judge.py`)
  - 计算结果：
  - Similarity between Xin1 & Zbn1 is  0.06187018230241209
  - Similarity between Xin2 & Zbn2 is  0.06477892868222472
  - Similarity between Xin1 & Xin2 is  0.21237343691611554
  - Similarity between Zbn1 & Zbn2 is  0.230070775607958
  - 可见此系数可以明确反映两家新闻公司的不同